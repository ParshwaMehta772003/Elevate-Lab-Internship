import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_breast_cancer

# --- 1. Load and Prepare Dataset ---
print("--- 1. Loading and Preparing Dataset ---")
# Load the Breast Cancer Wisconsin (Diagnostic) dataset
cancer = load_breast_cancer()
X = pd.DataFrame(cancer.data, columns=cancer.feature_names)
y = cancer.target # 0 for benign, 1 for malignant

# Correcting the KeyError: The column names are already in X.columns
# The error "None of [Index(['radius_mean', 'texture_mean'], dtype='object')] are in the [columns]"
# suggests that the column names might not exactly match or there's an issue with how X is created.
# Let's print the actual column names to verify.
print("Available columns in X:", X.columns.tolist())

# Ensure the feature names are correct and match the dataset's actual feature names.
# The load_breast_cancer() function directly provides feature_names.
# Let's re-confirm the exact names.
# If the error persists, it might be due to a subtle difference in string representation.
# We can try to access them directly from cancer.feature_names if needed.

# For visualization purposes, we'll select two features: 'mean radius' and 'mean texture'
# These are the actual names from load_breast_cancer().
# The original code used 'radius_mean' and 'texture_mean', which are slightly different.
X_2d = X[['mean radius', 'mean texture']]


# Split the full dataset into training and testing sets
X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Split the 2D dataset for visualization purposes
X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(X_2d, y, test_size=0.3, random_state=42, stratify=y)

# Standardize the features (important for SVMs, especially with RBF kernel)
scaler_full = StandardScaler()
X_train_full_scaled = scaler_full.fit_transform(X_train_full)
X_test_full_scaled = scaler_full.transform(X_test_full)

scaler_2d = StandardScaler()
X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)
X_test_2d_scaled = scaler_2d.transform(X_test_2d)

print(f"Original data shape: {X.shape}")
print(f"2D data shape (for visualization): {X_2d.shape}")
print(f"Training data samples (full): {X_train_full_scaled.shape[0]}")
print(f"Testing data samples (full): {X_test_full_scaled.shape[0]}")
print("Dataset loaded and prepared successfully.\n")

# --- 2. Train an SVM with Linear and RBF Kernel ---
print("--- 2. Training SVMs with Linear and RBF Kernels ---")

# Initialize and train Linear SVM on the full scaled dataset
svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X_train_full_scaled, y_train_full)
linear_accuracy = svm_linear.score(X_test_full_scaled, y_test_full)
print(f"Linear SVM Test Accuracy: {linear_accuracy:.4f}")

# Initialize and train RBF SVM on the full scaled dataset
svm_rbf = SVC(kernel='rbf', random_state=42)
svm_rbf.fit(X_train_full_scaled, y_train_full)
rbf_accuracy = svm_rbf.score(X_test_full_scaled, y_test_full)
print(f"RBF SVM Test Accuracy (default params): {rbf_accuracy:.4f}\n")

# --- 3. Visualize Decision Boundary Using 2D Data ---
print("--- 3. Visualizing Decision Boundaries (using 2D data) ---")

def plot_decision_boundary(X_train, y_train, X_test, y_test, model, title, feature_names):
    # Create a mesh to plot in
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))

    # Predict on mesh points
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(10, 7))
    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

    # Plot training points
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, s=30, edgecolors='k', label='Training points')
    # Plot testing points
    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.coolwarm, s=50, edgecolors='k', marker='X', label='Test points')

    plt.xlabel(f'{feature_names[0]} (scaled)')
    plt.ylabel(f'{feature_names[1]} (scaled)')
    plt.title(title)
    plt.legend()
    plt.colorbar(label='Class')
    plt.show()

# Train linear SVM on 2D scaled data for visualization
svm_linear_2d = SVC(kernel='linear', random_state=42)
svm_linear_2d.fit(X_train_2d_scaled, y_train_2d)
plot_decision_boundary(X_train_2d_scaled, y_train_2d, X_test_2d_scaled, y_test_2d, svm_linear_2d, 'Linear SVM Decision Boundary (2D)', ['mean radius', 'mean texture'])

# Train RBF SVM on 2D scaled data for visualization
svm_rbf_2d = SVC(kernel='rbf', random_state=42)
svm_rbf_2d.fit(X_train_2d_scaled, y_train_2d)
plot_decision_boundary(X_train_2d_scaled, y_train_2d, X_test_2d_scaled, y_test_2d, svm_rbf_2d, 'RBF SVM Decision Boundary (2D)', ['mean radius', 'mean texture'])
print("Decision boundaries visualized for 2D data.\n")

# --- 4. Tune Hyperparameters like C and gamma ---
print("--- 4. Tuning Hyperparameters (C and gamma for RBF kernel) ---")

# Define the parameter grid for GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1],
    'kernel': ['rbf'] # We are specifically tuning the RBF kernel
}

# Create a GridSearchCV object
# cv=5 means 5-fold cross-validation
# verbose=2 provides more output during the search
# n_jobs=-1 uses all available CPU cores
grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, verbose=2, n_jobs=-1)

# Fit the grid search to the full scaled training data
grid_search.fit(X_train_full_scaled, y_train_full)

print(f"Best parameters found: {grid_search.best_params_}")
print(f"Best cross-validation score (mean accuracy): {grid_search.best_score_:.4f}")

# Evaluate the best model found by GridSearchCV on the unseen test set
best_svm_tuned = grid_search.best_estimator_
tuned_accuracy = best_svm_tuned.score(X_test_full_scaled, y_test_full)
print(f"Tuned RBF SVM Test Accuracy: {tuned_accuracy:.4f}\n")

# --- 5. Use Cross-Validation to Evaluate Performance ---
print("--- 5. Cross-Validation Performance Evaluation ---")

# Cross-validation for Linear SVM (on full scaled data)
# Note: We use the full scaled training dataset (X_train_full_scaled, y_train_full) for cross_val_score
# to evaluate the model's robustness on different subsets of the training data.
cv_scores_linear = cross_val_score(SVC(kernel='linear', random_state=42), X_train_full_scaled, y_train_full, cv=5)
print(f"Linear SVM Cross-Validation Scores (training set): {cv_scores_linear}")
print(f"Linear SVM Mean CV Accuracy (training set): {cv_scores_linear.mean():.4f}")

# Cross-validation for RBF SVM (default params, on full scaled data)
cv_scores_rbf_default = cross_val_score(SVC(kernel='rbf', random_state=42), X_train_full_scaled, y_train_full, cv=5)
print(f"\nRBF SVM Cross-Validation Scores (default params, training set): {cv_scores_rbf_default}")
print(f"RBF SVM Mean CV Accuracy (default params, training set): {cv_scores_rbf_default.mean():.4f}")

# Cross-validation for Tuned RBF SVM (on full scaled data)
# The best_svm_tuned model already has the optimal parameters from GridSearchCV
cv_scores_tuned_rbf = cross_val_score(best_svm_tuned, X_train_full_scaled, y_train_full, cv=5)
print(f"\nTuned RBF SVM Cross-Validation Scores (training set): {cv_scores_tuned_rbf}")
print(f"Tuned RBF SVM Mean CV Accuracy (training set): {cv_scores_tuned_rbf.mean():.4f}\n")

print("SVM Task Completed!")
